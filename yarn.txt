一、yarn介绍
1. 背景
	- yarn是负责集群资源调度的
	- hadoop1.x中只有hdfs负责存储，mapreduce负责所有计算相关（分为计算和调度），
		- jobtracker进程：负责资源调度（随机调度）和监控程序运行状态，以及程序的启动和停止
			- 存在单点故障问题
		- tasktracker进程: 负责计算程序的执行
			- 强行将计算资源分成两部分：mapslot和reduceslot，分别跑map任务和reduce任务，只能跑对应任务
	- 这种架构的缺陷
		- 单点故障：jobtracker宕机，程序就跑不下去
		- 资源调度随机的，会造成资源浪费
		- 造成jobtracker运行压力过大
	
	- 2.x版本将计算和资源调度分离，yarn专门负责资源调度
		- 运行的相关进程runjar\MRAppMaster\Yarnchild

2. yarn介绍
 1）yarn的架构是主从架构
	- yarn启动时会出现两个进程resourcemanager和nodemanager
		- resourcemanager是资源管理和调度，负责
			- 接收客户端运行程序的请求
			- 接收nodemanager的状态报告，包括nodemanager的资源状态和存活状态
			- 整个计算程序的资源调度：调度运行资源和分配在哪个节点上运行
		- nodemanager: 负责真正提供资源，运行计算程序
			- 接收resourcemanager的命令
			- 运行计算程序
 2）启动计算和资源调度
	- MRAppMaster: 单个计算程序的管理者，类似于整个计算程序的项目经理
		- 负责帮助当前计算程序向resourcemanager申请资源
		- 负责启动maptask和reducetask任务，并监控它们的运行进度
	- ASM(applications manager) : 所有应用程序的管理者
		- application对应的job
		- ASM是resoucemanager的组件
		- 负责调度应用程序
	- container: 是一个抽象的资源容器，封装了一定的cup、io和网络资源
		- 是maptask和reducetask等的运行资源单位
			- 1个task会启动一个container，在进程中就显示的是1个yarnchild进程
	- scheduler调度器
		- 是resourcemanager的组件
		- 调度的是什么时间执行哪个计算程序
		- yarn默认调度器有三种
			- FIFO：first in first out，先进先出（先提交的程序先执行，后提交的程序后执行）
			- FAIR：公平调度器，大家平分资源运行
			- CAPICITY：计算能力调度器，可以按需进行配置多个队列的资源。内部维护多个队列，不同队列之间可以进行资源分配，每个队列中都是执行先进先出FIFO

二、yarn的资源调度流程
	- 客户端向resourcemanager提交请求
		- 提交的实际上是一个hadoop jar命令
		- 首先是发送给resourcemanager的组件ASM
	- ASM将请求转发给scheluer，进行资源调度排队
	- scheduler为当前job分配资源，随机返回一个节点（如nodemanger01）
	- resourcemanager到nodemanager01上去开启一个container，在这个container开启一个mrappmaster
	- mrappmaster向resourcemanager申请资源，为maptask和reducetask申请资源
	- resouremanager向mrappmater返回有资源的节点
		- 返回节点的规则：优先返回有数据的节点（数据块所在的节点）
		- 称为数据本地化规则，可以避免计算过程中数据的网络传输，提升计算性能
	- mrappmaster接收到资源节点(如nodemanager02和nodemanager03)，到对应的两个节点上分别开启container, 再在container中开启maptask任务
	- 所有的maptask任务会随时向mrappmaster汇报自己的运行状态和进度（mrappmaster需要知道什么时候开启reducetask）
	- mrappmaster在所有maptask执行80%左右的时候开启reducetask的任务（开启也需要一定时间，所以留有20%的余地）
	- mrappmaster到对应节点开启container，在container中开启reducetask
	- reducetask向mrappmaster汇报自己的运行状态和进度
	- mrappmaster监控到所有maptask和reducetask执行完毕，进行容器回收和销毁
	- mrappmaster向resoucemanager注销自己

三、yarn的job提交过程
1. job提交的细节
	- 执行程序的时候maptask和reducetask每一个任务都在执行jar包，但是jar包提交的时候只上传到一个节点上，但task运行可能与jar包不在一个节点，但程序并没有报错，应该是maptask和reducetask运行的本地存在jar包
	- 每次运行job的时候，都会生成一个和job_id相同的目录，这个目录下存放的是运行程序所需的公共资源（maptask和reducetask运行过程中通用的资源），包含三个核心文件：
		- jar包(job.jar)：运行程序的jar包
		- job.split： 切片信息，是通过程序FileInputFormat.addPath()，getSplits()
		- job.xml： 配置文件信息，封装job运行的参数
	- 三个核心文件在程序运行中会提交到hdfs，程序运行完成会被删除。
	- 运行过程中maptask和reducetask运行节点如果需要，就会从hdfs拉取到本地，存放到本地配置文件

2. job提交过程（resourcemanager-rm, nodemanager-nm）
	- client向rm提交job请求（hadoop jar.....）, jar包在客户端
	- rm向client返回一个共享资源路径和一个application_id
	- client拿到共享资源路径后，将运行程序需要的共享资源放置到共享资源路径上
	- client向rm发送资源放置成功的报告，并开始提交应用程序
	- rm接收到运行程序的请求，返回一个空闲的资源节点nm02
	- rm到对应的资源节点nm02上启动container
	- 初始化应用程序，生成一个应用程序的作业簿，用于记录maptask和reducetask的运行状态和进度信息
	- 在container中启动mrappmaster
	- mrappmaster到hdfs的共享资源路径下获取切片信息和配置文件信息（才知道要启动多少个maptask和reducetask）
	- mrappmaster向rm发送运行maptask和reducetask的资源申请
	- rm在处理mrappmaster的请求时，向mrappmaster返回空闲节点，运行maptask和reducetask
		- 优先处理maptask的请求
		- 数据本地优先原则
		- 返回nm03和nm04
	- 各节点到共享资源路径上下载运行maptask所需的jar包和配置信息等
	- mrappmaster到对应节点上启动container,在container上启动maptask
	- maptask向mrappmaster汇报自己的运行状况和进度
	- 当mrappmaster监控到maptask运行80%，启动reducetask，启动之前会去hdfs共享资源路径下下载jar包和配置信息
	- reducetask与mrappmaster保持通信，汇报自身的运行状态和进度
	- 运行过程中，当maptask运行完成，向mrappmaster注销自己；
	- 当mrappmaster监控到所有maptask和reducetask运行完成，向resourcemanager注销自己，进行资源回收
	
四、yarn集群上各进程/程序的作用
（1）主节点resourcemanager的职责
	- 处理客户端请求
	- 启动和监控mrappmaster
	- 监控nodemanager
	- 资源分配、调度和回收
（2）从节点nodemanager的职责：真正的资源提供者和应用程序容器的提供者，监控应用程序的资源使用情况(cup\内存\硬盘\网络),并通过心跳向resourcemanager汇报，也会监督container的资源自用情况，进行生命周期管理
	- 管理单个节点上的资源
	- 处理来自resourcemanager的命令
	- 处理来自mrappmaster的命令
（3）mrappmaster：对应一个应用程序
	- 向资源调度器申请执行任务的容器资源，运行并监控整个任务的执行，跟踪任务状态，处理任务失败进行重启
（4）container: 是一个抽象出来的逻辑资源单位，包含了节点上一定的cpu、内存、硬盘、网络等资源，mapreduce程序的所有task都是在这个容器里完成，容器大小是可以动态调整的