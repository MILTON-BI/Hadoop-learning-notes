
一、介绍
1. 是什么：分布式文件系统
2. 设计思想：
（1）分块存储
	- 分而治之：大文件切分存储
	- 分块存储，每一个块叫block
	- 分块不能太大，否则会造成负载不均衡
	- hadoop2.x中默认的块大小是128M，hadoop.1x中默认64M
	- 如果一个文件不足128M，也会单独存一个块，这时块大小就是存储数据的实际大小
（2）备份机制
	- 如果有一个快的存储节点宕机，这时数据完整性就得不到保证
	- 所以hdfs中默认采用备份机制，默认备份个数3
	- 所有备份是相同地位，没有主次之分
	- 备份的数据块一定存储在不同的节点上
	- 如果节点数为2，设置的副本数3，则实际只会存储2个，另一个进行记账，当集群节点数大于等于2的时候会复制这个副本，保证最后达到3个

二、hdfs的架构
1. 主从架构：一个主节点，多个从节点
2. 进程：namenode,datanode,secondarynamenode
（1）namenode
	1）存储元数据，存贮以下的内容： 
	- 抽象目录树：hdfs的目录结构与linux类似，以“/”为根节点。这个目录结构代表所有数据节点抽象出来的目录，不代表任何一个具体的节点目录
	- 存储数据与数据块block的对应关系：hdfs中存储的block是有编号的；底层存储的时候，每个block都有唯一的ID
	- block存储的物理位置（存储在哪个节点上）	
	2）处理客户端的读写请求
（2）datanode
	1) 负责真正存储数据block
	2) 负责实际处理读写
	    - 读：下载；
	    - 写：上传
（3）secondarynamenode 冷备份节点
	1) 是namenode的冷备份节点：当namenode宕机的时候，secondarynamenode不会自动切换为namenode，但可以手动切换
		- 手动切换，需要修改profile配置文件
	2）存储的数据与namenode基本相同
	3）主要作用是namenode宕机后，帮助namenode恢复；另一个作用是帮助namenode做元数据合并，分担namenode的压力

三、hdfs的优缺点
1. 优点：
――可构建在廉价机器上（成本低）；
――高容错性（多个副本，副本丢失会自动恢复，最终恢复到用户设定的副本个数）；
――适合批处理，适合离线数据处理（计算向数据靠拢，不适合实时数据处理）；
――适合大数据（TB\PB级数据，百万规模以上文件数）；
――流式文件访问（一次性写入，写入之后不支持数据修改；多次读取，保证数据一致性），
2. 缺点
――不支持低延迟的数据访问，不支持实时、近实时数据，纯离线的
――不擅长存储大量小文件（kb级别）
	- 寻址时间太长，效率太低，不划算
	- 元数据存储量过大，增加namenode的压力
		- hdfs中一条元数据一般150byte左右
――不支持文件内容修改

四、hdfs的应用
（一）hdfs命令方式：
――hdfs是一个类似linux的文件系统，类似linux系统的命令方式
――hdfs文件系统中，文件的访问方式只有通过绝对路径访问
――在任何一个节点都可以访问抽象目录树下的文件，具体存储是在集群中的任意节点
――shell命令在集群中的任何一个节点都可以操作
（1）开启hadoop或hdfs的客户端
	- hadoop fs 开启原生文件系统客户端
		- 结束安全模式：hadoop dfsadmin -safemode leave
		- 在根下创建目录：hadoop fs -mkdir [-p] /test (-p表示级联创建多级目录)
			- / 具体表示的是namenode下面的根目录：hdfs://hadoopm:9000/
			- test文件具体表示是hdfs://hadoopm:9000/test/
		- 察看根目录下文件/文件夹：hadoop fs -ls /
	- 或hdfs fs
（2）上传hadoop fs -put [-f] [-p] [-l] <localsrc> ... <dst> （可以上传多个文件，用空格分开）
	- localsrc: 本地文件目录，可以使用相对路径
	- dst ：目标目录，即hdfs上面的目录
		- 示例：（进入相应的本地文件目录）hadoop fs -put wordcount_test.txt /test/test1
（3）下载hadoop fs -get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>   （可以下载多个文件）
	- 示例：hadoop fs -get  /test/test1/wordcount_test.txt  /usr/local/hadoop
（4）getmerge：合并下载多个文件（将相同类型的文件合并输出），并合并生成一个新文件
	- 示例：hadoop fs -getmerge /aaa/log.*  ./log.sum  把hdfs目录aaa下的多个log文件下载到本地
	- 如果本地文件下载文件的文件夹不存在，则会创建
（5）察看文件内容-cat
	- 示例：hadoop fs -cat /test/test1/test2/test2.txt
（6）删除文件-rm -r -f (-r 递归删除，删除路径才需要；-f 强制删除)
	- 实例：hadoop fs -rm -f  /test/test1/wordcount_test.txt
（7）移动hadoop fs -mv <src> ... <dst>	
	- 示例：hadoop fs -cp  /test/test1/test2/test3.txt  /test/test1/test2/aaa.txt 
（8）复制hadoop fs -cp [-f] [-p | -p[topax]] <src> ... <dst>
	- 示例：hadoop fs -mv /test/test1/test2/test3.txt  /test/test3.txt 
（9）删除空目录：hadoop fs -rmdir [--ignore-fail-on-non-empty] <dir> ...
	- 基本不用，因为rm都能做
（10）从本地剪切到hdfs: hadoop fs -moveFromLocal <localsrc> ... <dst>
	- 与put命令类似，但是操作后本地文件将不存在，put命令操作后本地的文件还存在
（11）从hdfs剪切到本地：hadoop fs -moveFromLocal <localsrc> ... <dst>
	- 与get命令类似，但是操作后hdfs文件将不存在，get命令操作后hdfs的文件还存在
	- 基本不用，因为相当于从hdfs系统删除文件
（12）文件追加：hadoop fs -appendToFile <localsrc> ... <dst>
	- 将本地文件追加到hdfs的文件中，在文件末尾追加
	- hdfs系统不支持文件修改，但支持文件内容追加
	- 追加的时候如果超过128M，会进行切块
	- 这个命令一般也不会使用，因为追加后，所有副本都需要改
（13）监听文件命令： hadoop fs -tail -f <file>
	- 显示文件末尾n行的信息
（14）-df: 统计文件夹的可用空间信息
          -du: 统计文件夹的大小信息
（15）权限管理命令
――linux的权限管理命令
	- 修改文件权限：chmod  文件权限（三个小于等于7的数字） 文件
	- 修改目录下所有文件权限：chmod  -R 文件权限（三个小于等于7的数字） 目录
		- 文件权限有三种：可读r-4、可写w-2、可执行x-1，最大7（可读可写可执行）
			- drwxr-xr-x ：
			- 第一段代表文件属性（d-目录，-代表文件，l-链接）；
			- 第二段代表权限（分为三组：第一个rw-代表本用户，第二个rw-代表本组用户，r--代表其他用户）。
				- 每组用户权限最大是值7，所以制定权限要制定三个小于等于7的数字
	- 修改文件所属用户和组：chown [-R]  用户名:组名  文件/目录   （R表示递归修改，即修改目录下所有文件所属用户和组）
――hadoop用法完全相同，只是在前面加hadoop fs	

（二）java API方式操作
需要java编程，暂时不考虑


五、hdfs的四大机制和两大核心功能
1. 概要
――hdfs提供的高容错性的分布式数据存储方法，依赖的是其四大机制和两大核心功能
――作为一个文件系统的两大核心功能是上传和下载

2. 四大机制
（0）hadoop集群启动时，各个进程启动的顺序(关闭时也一样)
	- 先启动namenode
	- 启动datanode
	- 启动secondarynamenode

（1）心跳机制
	- 集群节点之间必须做时间同步
		- 参数dfs.heartbeat.interval
	- namenode要做集群分工，必须知道datanode的存活状况，通过datanode定期向namenode发送心跳信息来确定的，datanode每隔3秒（默认）向namenode发送心跳报告，报告自己的存活状况。
	- 当namenode连续10次没有收到某datanode的心跳报告，则认为它可能死亡，但没有断定（可能是网络延迟等原因）
	- 这时namenode会主动向datanode发送一次检查包（发送一次检查的时间为5分钟），如果一次检查没有返回信息，则再进行一次检查，如果再次获取不到返回信息，则断定该datanode死亡
	- namenode最终判定datanode死亡所需要的时间=10*3s + 2*3m*60s = 630s

（2）安全模式
  1）元数据：
	- 包括：抽象目录树+数据和块的映射关系+数据块存储的位置信息
	- 存储位置有2个：
		- 如果存在磁盘，那么每次进行读写的时候，磁盘I/O是瓶颈，频繁读写的性能比较低。
		- 因此集群正常启动后，元数据就被读取到内存，读写速度快，但是一旦关机就会造成数据丢失
		- 所以元数据既在内存存储，又在磁盘存储
		- 但内存中存储元数据中的抽象目录树、数据和块的映射关系和数据库存储位置三部分。磁盘中存储只有1和2两部分
 2）集群启动的时候做了什么？
	- 首先启动namenode，这时会将磁盘中的元数据加载到内存中，如果磁盘中元数据过大会造成加载时间过长，所以磁盘中的元数据只存储了1和2两个部分
	- 启动datanode，加载到内存的元数据的第3部分是就通过datanode的心跳报告获取的，即集群启动时namenode会接收datanode的心跳报告，这个心跳报告包含了数据块的存储位置信息（还有datanode的存活状况）
	- 启动secondarynamenode
 3）集群在执行启动的过程中，不允许外界对集群进行操作，这时候集群处于安全模式。
	- 集群处于安全模式的时候，用于加载元数据和获取datanode的心跳信息
	- 集群处于维护或升级状况时，也可以手动将集群设置为安全模式状态
	- hdfs dfsadmin [-safemode enter | leave | get | wait]
		- hdfs dfsadmin -safemode enter 进入安全模式
		- hdfs dfsadmin -safemode leave 退出安全模式
		- hdfs dfsadmin -safemode get 获取安全模式的状态（安全模式是否开启，开启返回on，关闭返回off）
		- hdfs dfsadmin -safemode wait 等待自行退出安全模式
 	- 安全模式下用户可以执行的操作
		- 可以进行的操作：hadoop fs  -ls \ -cat \ -get下载 \， 只要不修改元数据的都可以
		- 不能进行的操作：hadoop fs -mkdir \ -put上传 \ 修改文件名 \ 文件追加 \ 移动文件

（3）机架策略（副本存放策略）
	- 放置服务器的架子称为机架。
		- 同一个机架上各节点服务器的通信，以及多个机架之间的通信是通过交换机
		- 同一个机架上的服务器使用同一个线路
	- 实质上是副本存放机制：默认3个副本，在机架上的存储机制
		- 第一份副本一般存储在客户端节点上（即第一个机架中上传文件的服务器节点）
		- 第二份副本存储在不同机架上的任意节点（防止同一个机架断电，数据访问不到）
		- 第三个副本，如果有第三个机架则存放在第三个机架上，如果只有两个机架则存放在与第一个副本存储的同一个机架的不同节点上（在风险度一样的情况下，优先选择网络传输少的）
	- 真实集群中，机架策略需要手动配置，配置哪些节点属于同一个机架。上面的策略是官方推荐的策略，真实集群可以根据实际情况自定义（考虑不同节点、不同机架、不同数据中心）

（4）负载均衡
	- 负载均衡：每个节点上存储的数据利用率百分比相差不大（能力越大，存储越多）
	- 文件上传时会优先选择客户端所在节点，如果经常使用同一个客户端，会造成客户端所在节点存储数据比较多
	- 但是集群会有自动进行负载均衡的操作，比较慢
		- 参数dfs.datanode.balance.bandwidthPerSec  限制负载均衡每秒带宽的，默认是1M/s，
		- 而且还需要在集群空闲的情况下进行每秒1M的负载均衡操作
		- 这种自动负载均衡对于小规模的集群是可以的，如果集群规模大的时候，花费的时间太长，需要手动进行
			- /sbin/start-balancer.sh 手动开启负载均衡，这个命令也不是立即执行，需要等到空闲的时候才执行
			- /sbin/stop-balancer.sh 停止负载均衡
		- 手动负载均衡时可以指定百分比参数
			- /sbin/start-balancer.sh -t n% 任意两个节点之间存储利用率的百分比相差不超过n%则认为负载均衡了
	- 负载均衡在集群中添加新节点的时候用的概率比较高

3. 两大核心功能
（1）数据上传
  1）上传步骤
	- 第一步：客户端向namenode发送数据上传-put命令和信息，信息中心包含文件大小的信息
	- 第二步：namenode接收到客户端的请求后会进行一系列的检查
		- 文件是否存在，如果文件存在会报错给客户端
		- 上传的父目录是否存在，不存在也报错
		- 检查客户端是否有上传的权限
		- 等等
	- 第三步：如果检查通过，namenode会根据文件大小进行切块和存储，并向客户端返回存储的节点信息（数据块的存储列表）
		- 返回节点按照就近原则，优先返回客户端所在节点，再返回同机架的节点，再返回不同机架的节点信息
		- 返回的信息包括：上传文件被切成几个块（和块名），以及每一块的副本分别存储在哪个节点上
	- 第四步：客户端接收到namenode返回的响应后，进行一次逻辑切块
		- 切块分为物理切块（真实的切分）和逻辑切块（概念上的划分）
		- 逻辑切块仅仅是偏移量的划分，成为逻辑切片规划
	- 第五步：客户端准备上传文件
	- 第六步：构建从客户端到相应datanode的上传通道pipeline
		- 构建是根据块id依次进行构建（按块id的序号），一个块的所有存储节点构建为一个数据流通道
			- 构建一条数据流通道：客户端 ―>datanode01 ―>datanode02 ―>datanode04
		- 构建完成后，各块存储的datanode依次返回响应信息（是否构建成功）
			- 响应信息传递：datanode04 ―>datanode02 ―>datanode01―>客户端
	- 第七步：进行文件上传
		- 进行上传的过程中进行文件切块，因为在上传过程中可以统计到已上传的大小信息，当达到第一个128M时切块
		- 上传是以pakage为单位上传，一个pakage是512KB
		- 上传按客户端 ―>datanode01 ―>datanode02 ―>datanode04，
			- 每个节点一边接受并写入磁盘，一边向下一个节点传输
			- 先写入节点的缓存中，每当缓存接收到一个完整的package，就向下一个节点传递
			- 同时缓存中的数据持续向节点的磁盘中写入
	- 第八步：当第一个块数据上传完成，传输通道关闭
	- 第九步开始：按上面6-8的步骤上传第二个块
	- 第十步：所有块上传完成，向客户端返回结果
	- 第十一步：所有上传完成，客户端向namenode返回一个状态信息，表示全部写入成功，或者失败的信息
	- 第十二步：namenode接收到客户端的信息，如果成功，就更新元数据信息

 2）文件上传过程中的异常
	- 如果有某个节点上传失败，hdfs会立即重试一次，如果再失败，则会将失败的节点从pipeline中剔除，并报告给namenode
	- hdfs只要至少一个节点上传成功就可以
	- 如果所有节点都失败，则客户端会向namenode重新申请节点，并重新构建pipeline
	- 文件上传过程中要保证至少一份副本上传成功，剩下的副本会在集群空闲的时候去成功的节点上进行异步复制

 3）数据上传的时候，一般情况下肯定会返回一个客户端所在节点（前提是客户端是其中一个数据节点），因为客户端所在节点不存在网络传输，失败的可能性小，即上传的时候优先选择客户端节点，可以保证数据至少上传到一个节点上

（2）数据下载
 1）下载步骤
	- 第一步：客户端向namenode发送-get命令，请求下载
	- 第二步：namenode在自己的元数据中查询
		- 查询到则向客户端返回数据的块号及副本存储节点
		- 查询不到会报错
	- 第三步：客户端进行第一个数据库的下载
		- 下载时，按照就近原则（先在本地下载，再到同机架的节点，再到不同机架的节点）
	- 第四步：第一个块下载完成进行checksum验证，生成crc校验文件，和上传时的.meta文件进行文件完整度校验（校验起始偏移量和结束偏移量之间的内容），校验通过则认为第一个块下载成功
	- 第五步：进行第二个块下载，重复3-4步骤
	- 第六步：所有块下载成功，客户端向namenode发送成功响应，namenode更新元数据

 2）文件下载中的异常
	- 数据块的某一个节点读取不到数据，会向namenode汇报，namenode会对这个节点做一个标记（问题节点）
	- 客户端接着读取这个块存储的其他节点

（3）附加：元数据管理
 1）包含：抽象目录树、块和数据的映射、数据块的存储位置
	- /hadoop/dfs目录下有三个文件夹
		- data: datanode存储数据的真实目录
		- name: namenode存储数据的真实目录，存储元数据信息
		- nm-local-dir: hdfs的本地缓存
	- 元数据存储目录下的文件分为五类
		- 历史日志文件：记录客户端对元数据操作，只记录操作信息（比如某一个用户对某一个目录进行了某一个操作）
			- 例如：edits_0000000000000000001-0000000000000000012
			- 历史日志文件是已经编辑好的日志文件
		- 正在编辑的日志文件：目前对元数据操作记录的文件
			- 例如：edits_inprogress_0000000000000000020
		- 镜像文件：
			- 就是真实的元数据信息经过序列化的文件（减小存储空间）
			- 集群启动的时候会加载这个文件，加载时会进行反序列化
			- 例如：fsimage_0000000000000000014，镜像文件序列化之后的文件
			            fsimage_0000000000000000014.md5 ，镜像文件序列化并经过md5加密后的文件
		- seen_txid：合并点记录文件
			- 空闲时通过合并日志文件修改镜像文件的过程就是合并过程
			- 合并点记录文件就是记录下一次需要合并的日志文件
		- VERSION：

 2）硬盘上存储的元数据
	- hdfs格式化的时候只有格式化的镜像文件和合并点记录文件，没有日志文件
	- 集群第一次启动，会生成一个正在编辑的日志文件
	- 硬盘上存储的元数据 = fsimage + 正在编辑的日志文件

 3）内存中的元数据
	- 元数据写入的机制
		- 先将操作写入到磁盘的正在编辑的日志文件
		- 再将操作写入内存中，就修改了内存中的元数据
	- 所以任何时候内存中保存的元数据都是最新的、最完整的元数据

 4）元数据的合并checkpoint
	- 元数据合并：fsimage + 正在编辑的日志文件	
	- 如果msiage不与正在编辑的日志文件进行合并，fsimage会与内存中的元数据差别越来越大，所以fsimage需要和日志文件定期合并，这个合并操作是secondarynamenode完成
	- secondary与namenode保持通信，监督是否达到合并条件，触发合并的条件如下（满足任何一个即可）：
		- 时间间隔dfs.namenode.checkpoint.period，默认是3600s（一小时）
		- 元数据条数dfs.namenode.checkpoint.txnx，默认是1000000条
	- checkpoint过程
		- secondarynamenode（snn）向namenode(nn)发送查询请求，是否需要checkpoint
		- 达到合并条件，nn向snn响应
		- snn向nn发送请求进行checkpoint
		- nn将正在编辑的日志文件进行回滚，将正在编辑的状态切换为编辑完成的状态
		- snn将nn上的fsimage和编辑完成的历史日志文件拉取到snn节点。如果第一次checkpoint，拉取的日志文件合并点记录的日志文件编号到最新回滚出来的日志文件之间的日志文件
		- snn进行合并：将edits和fsimage加载到snn的内存中，根据edits的操作日志改变fsimage的元数据信息
		- snn将合并后的fsimage文件(fsimage.checkpoint)发送回nn
			- snn也会在自己的磁盘上保存一份fsimage文件，进行永久保存
			- 为namenode做备份，以防namenode宕机，元数据丢失时帮助namenode恢复
			- 如果不是第一次checkpoint，snn只需要拉取日志文件，不再需要拉取fsimage，因为自己已存储了一份
		- nn将fsimage.checkpoint进行重命名fsimage，替换掉原始的fsimage文件

六、hdfs补充
1. namenode的作用
	- 保存元数据
	- 处理客户端的读写请求
	- 分配数据块的存储节点，进行负载均衡
2. secondarynamenode的作用
	- 帮助namenode做元数据备份，在namenode宕机的时候帮助进行数据恢复
	- 进行checkpoint，帮助namenode进行元数据合并，减轻namenode压力
3. datanode的作用
	- 用于存储数据块
	- 处理真正的读写
	- 定期向namenode发送心跳报告（生存状态信息，块的位置信息）
		- 数据块存储在datanode，datanode只知道存储了哪些块，并不知道这些块分别属于哪个文件，namenode才知道
		- namenode的元数据中不包含数据块存储位置的信息，只保存一个空列表，但包含文件和数据块的对应关系，块的位置信息是在加载到内存后，由datanode汇报后添加上的，所以只有内存上的才会保存块的位置信息
	